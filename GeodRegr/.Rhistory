return (answer)
}
loss_sum <- function(p,v,x,y,m_estimator) {
sum <- 0
res <- eps(p,v,x,y)
for (i in 1:dim(y)[2]) {
sum <- sum + rho(norm(res[,i]),m_estimator)
}
return (sum)
}
j_p <- function(p,v1,v2) {
if (norm(v1) != 0) {
j <- (0+1i)*v1
#v2_0 <- pt(expo(p,v1),-gammaprime(1,p,v1),v2)
v2_0 <- pt(expo(p,v1),p,v2)
#v2_0 <- reorient(expo(expo(p,v1),-gammaprime(1,p,v1)),v2_0,p)
#v2_0 <- pt(expo(p,v1),p,v2)
#u_0 <- (abs(sum(v2_0*Conj(j/norm(j)))))*(j/norm(j))
w_0 <- (Re(sum(v2_0*Conj(j/norm(j)))))*(j/norm(j))
#u_0 <- sum(v2_0*Conj(j/norm(j)))*(j/norm(j))
#u_0 <- sum(Conj(v2_0)*(j/norm(j)))*(j/norm(j))
u_0 <- v2_0-w_0
#u_0 <- pt(expo(p,v1),p,u)
#u_tan <- (abs(sum(u_0*Conj(v1/norm(v1)))))*(v1/norm(v1))
w_tan <- (Re(sum(w_0*Conj(v1/norm(v1)))))*(v1/norm(v1))
#u_tan <- (sum(u_0*Conj(v1/norm(v1))))*(v1/norm(v1))
#u_tan <- (sum(Conj(u_0)*(v1/norm(v1))))*(v1/norm(v1))
w_orth <- w_0-w_tan
#w_0 <- pt(expo(p,v1),-gammaprime(1,p,v1),w)
#w_tan <- (abs(sum(w_0*Conj(v1/norm(v1)))))*(v1/norm(v1))
u_tan <- (Re(sum(u_0*Conj(v1/norm(v1)))))*(v1/norm(v1))
#w_tan <- (sum(w_0*Conj(v1/norm(v1))))*(v1/norm(v1))
#w_tan <- (sum(Conj(w_0)*(v1/norm(v1))))*(v1/norm(v1))
u_orth <- u_0-u_tan
L <- norm(v1)
result <- cos(L)*u_orth + cos(2*L)*w_orth + u_tan + w_tan
} else {
result <- v2
}
return (result)
}
j_v <- function(p,v1,v2) {
if (norm(v1) != 0) {
j <- (0+1i)*v1
#v2_0 <- pt(expo(p,v1),-gammaprime(1,p,v1),v2)
v2_0 <- pt(expo(p,v1),p,v2)
#v2_0 <- reorient(expo(expo(p,v1),-gammaprime(1,p,v1)),v2_0,p)
#v2_0 <- pt(expo(p,v1),p,v2)
#u_0 <- (abs(sum(v2_0*Conj(j/norm(j)))))*(j/norm(j))
w_0 <- (Re(sum(v2_0*Conj(j/norm(j)))))*(j/norm(j))
#u_0 <- sum(v2_0*Conj(j/norm(j)))*(j/norm(j))
#u_0 <- sum(Conj(v2_0)*(j/norm(j)))*(j/norm(j))
u_0 <- v2_0-w_0
#u_0 <- pt(expo(p,v1),p,u)
#u_tan <- (abs(sum(u_0*Conj(v1/norm(v1)))))*(v1/norm(v1))
w_tan <- (Re(sum(w_0*Conj(v1/norm(v1)))))*(v1/norm(v1))
#u_tan <- (sum(u_0*Conj(v1/norm(v1))))*(v1/norm(v1))
#u_tan <- (sum(Conj(u_0)*(v1/norm(v1))))*(v1/norm(v1))
w_orth <- w_0-w_tan
#w_0 <- pt(expo(p,v1),-gammaprime(1,p,v1),w)
#w_tan <- (abs(sum(w_0*Conj(v1/norm(v1)))))*(v1/norm(v1))
u_tan <- (Re(sum(u_0*Conj(v1/norm(v1)))))*(v1/norm(v1))
#w_tan <- (sum(w_0*Conj(v1/norm(v1))))*(v1/norm(v1))
#w_tan <- (sum(Conj(w_0)*(v1/norm(v1))))*(v1/norm(v1))
u_orth <- u_0-u_tan
L <- norm(v1)
result <- ((sin(L))/L)*u_orth + ((sin(2*L))/(2*L))*w_orth + u_tan + w_tan
} else {
result <- v2
}
return (result)
}
grad_p <- function(p,v,x,y,m_estimator) {
sum <- integer(length(p))
res <- eps(p,v,x,y)
if (dim(x)[2]==1) {
for (i in 1:dim(y)[2]) {
if (norm(res[,i])!=0) {
sum <- sum - rho_prime(norm(res[,i]),m_estimator)*j_p(p,x[i]*as.vector(v),(res[,i]/norm(res[,i])))
}
}
} else {
shifts <- v%*%t(x)
for (i in 1:dim(y)[2]) {
if (norm(res[,i])!=0) {
sum <- sum - rho_prime(norm(res[,i]),m_estimator)*pt(expo(p, shifts[,i]),p,(res[,i]/norm(res[,i])))
}
}
}
return (sum)
}
grad_v <- function(p,v,x,y,m_estimator) {
sum <- matrix(0L,nrow=length(p),ncol=dim(x)[2])
res <- eps(p,v,x,y)
if (dim(x)[2]==1) {
for (i in 1:dim(y)[2]) {
if (norm(res[,i])!=0) {
sum <- sum - x[i]*rho_prime(norm(res[,i]),m_estimator)*j_v(p,x[i]*as.vector(v),(res[,i]/norm(res[,i])))
}
}
} else {
shifts <- v%*%t(x)
for (h in 1:dim(x)[2]) {
for (i in 1:dim(y)[2]) {
if (norm(res[,i])!=0) {
sum[,h] <- sum[,h] - x[i,h]*rho_prime(norm(res[,i]),m_estimator)*pt(expo(p, shifts[,i]),p,(res[,i]/norm(res[,i])))
}
}
}
}
return (sum)
}
#reorient <- function(p1,v,p2) { ### p1 and p2 belong to same equivalence class, move tangent vector from p1 to p2
#  return (loga(p2,expo(p1,v)))
#}
alg <- function(p,v,x,y,m_estimator) {
p <- (p-mean(p))/norm(p-mean(p))
current_p <- p
current_v <- v
old_p <- integer(length(p))
old_p[1] <- 0.5^0.5
old_p[dim(y)[1]] <- -(0.5^0.5)
old_v <- matrix(0L,nrow=length(p),ncol=dim(x)[2])
##copy_old_p <- old_p
##copy_old_v <- old_v
count <- 0
alt_count <- 0
if ((m_estimator[[1]] == 'huber') | (m_estimator[[1]] == 'tukey')) {
xi <- (2*Pinv(dim/2,0.5))^0.5
deviations <- vector(length=dim(y)[2])
current_shifts <- current_v%*%t(x)
for (i in 1:dim(y)[2]) {
deviations[i] <- dist(expo(current_p,current_shifts[,i]),y[,i])
}
mad <- median(deviations)
if (m_estimator[[1]] == 'huber') {
c <- nr(2,(2*length(p)-4),m_estimator)
} else if (m_estimator[[1]] == 'tukey') {
#c <- nr(8,(2*length(p)-4),m_estimator)
c <- nr(15,(2*length(p)-4),m_estimator)
}
sigma <- mad/xi
cutoff <- c*sigma
rho_function <- m_estimator
m_estimator <- vector('list',length=2)
m_estimator[[1]] <- rho_function
m_estimator[[2]] <- cutoff
}
step_p <- grad_p(current_p,current_v,x,y,m_estimator)
step_v <- grad_v(current_p,current_v,x,y,m_estimator)
v_diffs <- vector(length=dim(x)[2])
for (h in 1:dim(x)[2]) {
v_diffs[h] <- norm((pt(old_p,current_p,old_v[,h])-current_v[,h]))
}
lambda <- min((1/norm(step_p)),0.1)
while ((count==0) | ((count<20000) & (alt_count<100000) & ((dist(old_p,current_p)>0.0000000001) | (any(v_diffs>0.0000000001))))) { ##& ((dist(copy_old_p,current_p)!=0) | (norm(current_v-copy_old_v)!=0)))) {
new_p <- expo(current_p, -lambda*step_p)
new_v <- matrix(,nrow=length(p),ncol=dim(x)[2])
for (h in 1:dim(x)[2]) {
new_v[,h] <- pt(current_p,new_p,current_v[,h]-lambda*step_v[,h])
}
if (loss_sum(current_p,current_v,x,y,m_estimator) >= loss_sum(new_p,new_v,x,y,m_estimator)) {
alt_count <- 0
##copy_old_p <- old_p
##copy_old_v <- old_v
old_p <- current_p
old_v <- current_v
current_p <- new_p
current_v <- new_v
if ((m_estimator[[1]] == 'huber') | (m_estimator[[1]] == 'tukey')) {
##xi <- (2*Pinv(dim/2,0.5*P(dim/2,0.5*(pi/sigma)^2)))^0.5
current_shifts <- current_v%*%t(x)
for (i in 1:dim(y)[2]) {
deviations[i] <- dist(expo(current_p,current_shifts[,i]),y[,i])
}
mad <- median(deviations)
##old_c <- c
##c <- trunc_nr(old_c,dim,pi/sigma,m_estimator[[1]])
##if (c == 'fail') {
##  c <- old_c
##}
sigma <- mad/xi
cutoff <- c*sigma
m_estimator[[2]] <- cutoff
}
step_p <- grad_p(current_p,current_v,x,y,m_estimator)
step_v <- grad_v(current_p,current_v,x,y,m_estimator)
for (h in 1:dim(x)[2]) {
v_diffs[h] <- norm((pt(old_p,current_p,old_v[,h])-current_v[,h]))
}
lambda <- min((1/norm(step_p)),2*lambda)
count <- count+1
} else {
lambda <- lambda/2
alt_count <- alt_count+1
}
}
result <- vector("list", length=3)
result[[1]] <- current_p
result[[2]] <- current_v
result[[3]] <- count
#result[[4]] <- alt_count
#result[[5]] <- lambda
#if ((m_estimator[[1]] == 'huber') | (m_estimator[[1]] == 'tukey')) {
#  result[[6]] <- sigma
#  result[[7]] <- cutoff
#  result[[8]] <- xi
#  result[[9]] <- c
#}
return (result)
}
## Karcher mean on Kendall's 2d shape space
k_mean_loss_sum <- function(p,y) {
sum <- 0
for (i in 1:dim(y)[2]) {
sum <- sum + 0.5*((dist(p,y[,i]))^2)
}
return (sum)
}
k_mean_grad <- function(p,y) {
sum <- integer(length(p))
for (i in 1:dim(y)[2]) {
sum <- sum - loga(p,y[,i])
}
return (sum)
}
k_mean <- function(y) {
current_p <- y[,1]
old_p <- y[,2]
lambda <- 0.1
step_p <- k_mean_grad(current_p,y)
count <- 0
while ((count==0) | (dist(old_p,current_p)>0.0000000001)) {
lambda <- min((1/norm(step_p)),lambda)
new_p <- expo(current_p, -lambda*step_p)
if (k_mean_loss_sum(current_p,y) >= k_mean_loss_sum(new_p,y)) {
##alt_count <- 0
old_p <- current_p
current_p <- new_p
step_p <- k_mean_grad(current_p,y)
lambda <- 2*lambda
count <- count+1
} else {
lambda <- lambda/2
}
}
result <- current_p
return (result)
}
#data
library(mvtnorm)
library(zipfR)
library(MASS)
library(R.matlab)
library(rmatio)
#library(abind)
#library(data.table)
#initializations
boundary_points <- 50
dim <- 2*boundary_points-4
embed <- boundary_points
n <- 1
estimator <- 'l2'
# data
#NCF_x_data <- readMat('../downloads/ADNI_RMRSS/data/NCinfoF.mat')$NCinfoF
#NCM_x_data <- readMat('../downloads/ADNI_RMRSS/data/NCinfoM.mat')$NCinfoM
ADF_x_data <- readMat('../downloads/ADNI_RMRSS/data/ADinfoF.mat')$ADinfoF
#ADM_x_data <- readMat('../downloads/ADNI_RMRSS/data/ADinfoM.mat')$ADinfoM
#NCF_y_data <- readMat('../downloads/ADNI_RMRSS/data/NCLdataF.mat')$NCLdataF
#NCM_y_data <- readMat('../downloads/ADNI_RMRSS/data/NCLdataM.mat')$NCLdataM
ADF_y_data <- readMat('../downloads/ADNI_RMRSS/data/ADLdataF.mat')$ADLdataF
#ADM_y_data <- readMat('../downloads/ADNI_RMRSS/data/ADLdataM.mat')$ADLdataM
ages <- ADF_x_data[,9] ###############################################
x_data <- t(t(ages))
x_data <- x_data-mean(x_data) ## centering
#x_data <- c(x_data,0)
y_data <- ADF_y_data
y_data <- aperm(y_data, c(2,1,3))
#y_data[,49:50,((length(ages)-19):length(ages))] <- 9999 ########### outliers
for (i in 1:length(ages)) { ## remove translation
for (j in 1:2) {
y_data[j,,i] <- y_data[j,,i]-mean(y_data[j,,i])
}
}
#y_data <- y_data[,1:embed,] ## remove last point, since it is determined by the first 49 points
for (i in 1:length(ages)) { ## remove scaling
y_data[,,i] <- y_data[,,i]/((sum(y_data[,,i]*y_data[,,i]))^0.5)
}
#y_data[2,,(length(ages)-19):length(ages)] <- -y_data[2,,(length(ages)-19):length(ages)] ### fake points
#fake_point <- vector(length=boundary_points)
#fake_point[1] <- exp((pi/boundary_points)*((-1+0i)^0.5))
#for (i in 2:boundary_points) {
#  fake_point[i] <- (fake_point[1])^i
#}
#fake_point <- ((1/boundary_points)^0.5)*fake_point
#fake_point <- rbind(Re(fake_point),Im(fake_point))
#y_data <- abind(y_data,fake_point)
#target <- y_data[,,1]
#for (i in 1:length(ages)) {
#  decomp <- svd(target%*%t(y_data[,,i]))
#  R <- (decomp$u)%*%t(decomp$v)
#  y_data[,,i] <- R%*%y_data[,,i]
#}
#count <- 0
#old_target <- -y_data[,,1] ## remove rotation, general procrustes analysis
#current_target <- y_data[,,1]
#while ((sum((current_target-old_target)*(current_target-old_target))^0.5)>0.00000001) {
#  for (i in 1:length(ages)) {
#    decomp <- svd(current_target%*%t(y_data[,,i]))
#    R <- (decomp$u)%*%t(decomp$v)
#    y_data[,,i] <- R%*%y_data[,,i]
#  }
#  old_target <- current_target
#  current_target <- rowSums(y_data,dims=2)
#  current_target <- current_target/((sum(current_target*current_target))^0.5)
#  count <- count+1
#}
y_data <- y_data[1,,]+y_data[2,,]*(0+1i)
##y_data <- rbind(Re(y_data),Im(y_data))
#fake_point <- vector(length=boundary_points)
#fake_point[1] <- exp((2*pi/boundary_points)*(0+1i))
#for (i in 2:boundary_points) {
#  fake_point[i] <- (fake_point[1])^i
#}
#fake_point <- ((1/boundary_points)^0.5)*fake_point
#y_data[,(length(ages)-19):length(ages)] <- fake_point
## solutions
##init_p <- expo(y_data[,7],0.5*loga(y_data[,7],y_data[,12]))
##init_p <- expo(y_data[,1],0.5*loga(y_data[,1],y_data[,2]))
init_p <- k_mean(y_data)
init_v <- t(t(integer(embed)))
ans <- alg(init_p,init_v,x_data,y_data,estimator)
total_var <- k_mean_loss_sum(k_mean(y_data),y_data)
unexp_var <- loss_sum(ans[[1]],ans[[2]],x_data,y_data,'l2')
r_sq <- 1-unexp_var/total_var
ans
document()
library(devtools)
document()
document()
devtools::document()
getwd
getwd()
setwd("C:/Users/hc.gong/Downloads")
document()
setwd("C:/Users/hc.gong")
document()
setwd("C:/Users/hc.gong/Downloads/GeodRegr")
document()
setwd("C:/Users/hc.gong/Downloads/GeodRegr/GeodRegr")
document()
rm(list=ls())
document()
(dim(contam_y_data)[1] - 19):dim(contam_y_data)[1]
document()
document()
document()
document()
document()
document()
document()
document()
document()
document()
document()
library(GeodRegr)
data(calvaria)
manifold <- 'kendall'
contam_x_data <- calvaria$x
contam_mean_x <- mean(contam_x_data)
contam_x_data <- contam_x_data - contam_mean_x # center x data
uncontam_x_data <- calvaria$x[ -c(23, 101, 104, 160)]
uncontam_mean_x <- mean(uncontam_x_data)
uncontam_x_data <- uncontam_x_data - uncontam_mean_x # center x data
contam_y_data <- calvaria$y
uncontam_y_data <- calvaria$y[, -c(23, 101, 104, 160)] # remove corrupted
# columns
landmarks <- dim(contam_y_data)[1]
dimension <- 2 * landmarks - 4
# we ignore Huber's estimator as the L_1 estimator already has an
# (approximate) efficiency above 95\% in 12 dimensions; see documentation for
# the are and are_nr functions
tol <- 1e-5
uncontam_l2 <- geo_reg(manifold, uncontam_x_data, uncontam_y_data,
'l2', p_tol = tol, V_tol = tol)
contam_l2 <- geo_reg(manifold, contam_x_data, contam_y_data,
'l2', p_tol = tol, V_tol = tol)
contam_l1 <- geo_reg(manifold, contam_x_data, contam_y_data,
'l1', p_tol = tol, V_tol = tol)
contam_tukey <- geo_reg(manifold, contam_x_data, contam_y_data,
'tukey', are_nr('tukey', dimension, 10, 0.99), p_tol = tol, V_tol = tol)
geodesics <- vector('list')
geodesics[[1]] <- uncontam_l2
geodesics[[2]] <- contam_l2
geodesics[[3]] <- contam_l1
geodesics[[4]] <- contam_tukey
loss(manifold, geodesics[[1]]$p, geodesics[[1]]$V, uncontam_x_data,
uncontam_y_data, 'l2')
loss(manifold, geodesics[[2]]$p, geodesics[[2]]$V, contam_x_data,
contam_y_data, 'l2')
loss(manifold, geodesics[[3]]$p, geodesics[[3]]$V, contam_x_data,
contam_y_data, 'l1')
loss(manifold, geodesics[[4]]$p, geodesics[[4]]$V, contam_x_data,
contam_y_data, 'tukey', are_nr('tukey', dimension, 10, 0.99))
# visualization of each geodesic
par(mfrow = c(1, 4))
days <- c(7, 14, 21, 30, 40, 60, 90, 150)
pal <- colorRampPalette(c("blue", "red"))(length(days))
# each predicted geodesic will be represented as a sequence of the predicted
# shapes at each of the above ages, the blue contour will show the predicted
# shape on day 7 and the red contour the predicted shape on day 150
contour <- vector('list')
for (i in 1:length(days)) {
contour[[i]] <- exp_map(manifold, geodesics[[1]]$p, (days[i] -
uncontam_mean_x) * geodesics[[1]]$V)
contour[[i]] <- c(contour[[i]], contour[[i]][1])
}
plot(Re(contour[[length(days)]]), Im(contour[[length(days)]]), type = 'n',
xaxt = 'n', yaxt = 'n', ann = FALSE, asp = 1)
for (i in 1:length(days)) {
lines(Re(contour[[i]]), Im(contour[[i]]), col = pal[i])
}
for (j in 2:4) {
for (i in 1:length(days)) {
contour[[i]] <- exp_map(manifold, geodesics[[j]]$p, (days[i] -
contam_mean_x) * geodesics[[j]]$V)
contour[[i]] <- c(contour[[i]], contour[[i]][1])
}
plot(Re(contour[[length(days)]]), Im(contour[[length(days)]]), type = 'n',
xaxt = 'n', yaxt = 'n', ann = FALSE, asp = 1)
for (i in 1:length(days)) {
lines(Re(contour[[i]]), Im(contour[[i]]), col = pal[i])
}
}
# even with a mere 4 corrupted landmarks out of a total of 8 * 168 = 1344, we
# can clearly see that contam_l2, the second image, looks slightly
# different from all the others, especially near the top of the image.
par
?mar
?par
data(calvaria)
manifold <- 'kendall'
contam_x_data <- calvaria$x
contam_mean_x <- mean(contam_x_data)
contam_x_data <- contam_x_data - contam_mean_x # center x data
uncontam_x_data <- calvaria$x[ -c(23, 101, 104, 160)]
uncontam_mean_x <- mean(uncontam_x_data)
uncontam_x_data <- uncontam_x_data - uncontam_mean_x # center x data
contam_y_data <- calvaria$y
uncontam_y_data <- calvaria$y[, -c(23, 101, 104, 160)] # remove corrupted
# columns
landmarks <- dim(contam_y_data)[1]
dimension <- 2 * landmarks - 4
# we ignore Huber's estimator as the L_1 estimator already has an
# (approximate) efficiency above 95\% in 12 dimensions; see documentation for
# the are and are_nr functions
tol <- 1e-5
uncontam_l2 <- geo_reg(manifold, uncontam_x_data, uncontam_y_data,
'l2', p_tol = tol, V_tol = tol)
contam_l2 <- geo_reg(manifold, contam_x_data, contam_y_data,
'l2', p_tol = tol, V_tol = tol)
contam_l1 <- geo_reg(manifold, contam_x_data, contam_y_data,
'l1', p_tol = tol, V_tol = tol)
contam_tukey <- geo_reg(manifold, contam_x_data, contam_y_data,
'tukey', are_nr('tukey', dimension, 10, 0.99), p_tol = tol, V_tol = tol)
geodesics <- vector('list')
geodesics[[1]] <- uncontam_l2
geodesics[[2]] <- contam_l2
geodesics[[3]] <- contam_l1
geodesics[[4]] <- contam_tukey
loss(manifold, geodesics[[1]]$p, geodesics[[1]]$V, uncontam_x_data,
uncontam_y_data, 'l2')
loss(manifold, geodesics[[2]]$p, geodesics[[2]]$V, contam_x_data,
contam_y_data, 'l2')
loss(manifold, geodesics[[3]]$p, geodesics[[3]]$V, contam_x_data,
contam_y_data, 'l1')
loss(manifold, geodesics[[4]]$p, geodesics[[4]]$V, contam_x_data,
contam_y_data, 'tukey', are_nr('tukey', dimension, 10, 0.99))
# visualization of each geodesic
oldpar <- par(mfrow = c(1, 4))
days <- c(7, 14, 21, 30, 40, 60, 90, 150)
pal <- colorRampPalette(c("blue", "red"))(length(days))
# each predicted geodesic will be represented as a sequence of the predicted
# shapes at each of the above ages, the blue contour will show the predicted
# shape on day 7 and the red contour the predicted shape on day 150
contour <- vector('list')
for (i in 1:length(days)) {
contour[[i]] <- exp_map(manifold, geodesics[[1]]$p, (days[i] -
uncontam_mean_x) * geodesics[[1]]$V)
contour[[i]] <- c(contour[[i]], contour[[i]][1])
}
plot(Re(contour[[length(days)]]), Im(contour[[length(days)]]), type = 'n',
xaxt = 'n', yaxt = 'n', ann = FALSE, asp = 1)
for (i in 1:length(days)) {
lines(Re(contour[[i]]), Im(contour[[i]]), col = pal[i])
}
for (j in 2:4) {
for (i in 1:length(days)) {
contour[[i]] <- exp_map(manifold, geodesics[[j]]$p, (days[i] -
contam_mean_x) * geodesics[[j]]$V)
contour[[i]] <- c(contour[[i]], contour[[i]][1])
}
plot(Re(contour[[length(days)]]), Im(contour[[length(days)]]), type = 'n',
xaxt = 'n', yaxt = 'n', ann = FALSE, asp = 1)
for (i in 1:length(days)) {
lines(Re(contour[[i]]), Im(contour[[i]]), col = pal[i])
}
}
# even with a mere 4 corrupted landmarks out of a total of 8 * 168 = 1344, we
# can clearly see that contam_l2, the second image, looks slightly
par(oldpar)
plot(1:1000,1:1000)
